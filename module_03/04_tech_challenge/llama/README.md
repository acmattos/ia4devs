# Fine-Tuning de LLM com Unsloth e LoRA

## Vis√£o Geral do Processo

Este documento descreve o processo de fine-tuning realizado em um modelo de linguagem (LLM). O c√≥digo ajusta um modelo pr√©-treinado (como Llama-3-8B) para gerar descri√ß√µes de produtos com base em instru√ß√µes estruturadas, uitilizando: 

- **Unsloth** para otimiza√ß√£o de desempenho
- **LoRA** para adapta√ß√£o eficiente
- **SFT** (Supervised Fine-Tuning)

## Fluxo Principal
1. Configura√ß√£o do ambiente
2. Carregamento do modelo e tokenizador
3. Aplica√ß√£o de LoRA
4. Prepara√ß√£o do dataset
5. Treinamento supervisionado
6. Infer√™ncia e salvamento

---

# 1. Prepara√ß√£o do Ambiente

Este trabalho foi planejado para rodar no **Windows**. Portanto, √© necess√°rio atentar-se √† instala√ß√£o correta das bibliotecas e depend√™ncias.

### Configura√ß√£o do Ambiente:

( Configura√ß√£o utilizada nos testes)
- **Sistema Operacional:** Windows 11
- **Python:** 3.12.6
- **GPU:** RTX 4060
- **CUDA:** 12.6
- **cuDNN:** 9.8

### Passos de Instala√ß√£o (executar na ordem):

1. [CUDA Toolkit 12.6](https://developer.nvidia.com/cuda-12-6-3-download-archive?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local)
2. [cuDNN 9.8.0 Downloads](https://developer.nvidia.com/cudnn-downloads)
3. [triton-windows](https://github.com/woct0rdho/triton-windows)
4. [PyTorch](https://pytorch.org/get-started/locally/)
5. [Unsloth](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation)

### Observa√ß√£o sobre o Unsloth:

A vers√£o `2025.2.X` apresentou problemas. Foi utilizada a vers√£o `2025.1.5` neste teste:

```bash
pip install unsloth==2025.1.5
```

A vers√£o de mar√ßo foi testada em outra m√°quina e funciona tamb√©m:

```bash
pip install --no-deps "unsloth>=2025.3.8" "unsloth_zoo>=2025.3.7" --upgrade --force-reinstall
```

---

## 2. Processamento dos Dados

### **Fonte dos Dados**

Os dados utilizados foram obtidos do [Amazon Titles Reasoning](https://huggingface.co/datasets/rickwalking/amazon-titles-reasoning).

### **Estrutura dos Dados**

Trabalhamos com o arquivo `trn.json`, que cont√©m os dados de treino no formato:

```json 
{"uid": "0001360000", "title": "Mog's Kittens", "content": "Judith Kerr&#8217;s best&#8211;selling adventures of that endearing (and exasperating) cat Mog have entertained children for more than 30 years. Now, even infants and toddlers can enjoy meeting this loveable feline. These sturdy little board books&#8212;with their bright, simple pictures, easy text, and hand&#8211;friendly formats&#8212;are just the thing to delight the very young. Ages 6 months&#8211;2 years.", "target_ind": [146, 147, 148, 149, 495], "target_rel": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"uid": "0000031895", "title": "Girls Ballet Tutu Neon Blue", "content": "Dance tutu for girls ages 2-8 years. Perfect for dance practice, recitals and performances, costumes or just for fun!", "target_ind": [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 31, 33, 42, 46, 54, 58, 111, 113, 125, 126, 159, 163, 202, 203, 204, 205, 206, 207], "target_rel": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
```

O arquivo `trn.json` cont√©m **2.248.619 registros**. Para o fine-tuning, utilizamos apenas os campos `title` e `content` e os demais campos ser√£o descartados.

### **Processamento dos Dados**

O arquivo `process_data.py` realiza o processamento dos dados: 
  1. **Filtragem:** Registros com `content` vazio ou <100 caracteres s√£o descartados.
  2. **Gera√ß√£o do dataset final:** Criado o arquivo `trn_processed.json` com **1.216.560 registros v√°lidos**.

Vamos descartar os registros com `content` vazio ou com o conte√∫do menor do que 100 caracteres.
Com isto, geramos o arquivo `trn_processed.json`, com 1.216.560 registros aptos 
a serem usados no treinamento do modelo.

### **Formato do Arquivo Processado:**

```json 
[
 {"product": "Mogs Kittens", "description": "Judith Kerr8217s best8211selling adventures of that endearing and exasperating cat Mog have entertained children for more than 30 years Now even infants and toddlers can enjoy meeting this loveable feline These sturdy little board books8212with their bright simple pictures easy text and hand8211friendly formats8212are just the thing to delight the very young Ages 6 months82112 years"},
 {"product": "Girls Ballet Tutu Neon Blue", "description": "Dance tutu for girls ages 28 years Perfect for dance practice recitals and performances costumes or just for fun"},
]
```

## Exemplo de execu√ß√£o:
``` 
(.venv) PS C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama> python .\process_data.py  
  
Lendo os registros do arquivo              : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2248619/2248619 [00:35<00:00, 62903.00it/s]  
Processando registros lidos                : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2248619/2248619 [00:14<00:00, 155615.25it/s]  
Salvando registros processado em arquivo   : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1216560/1216560 [00:10<00:00, 117362.03it/s]  
Total de registros no arquivo original     : 2,248,619  
Total de registros processados (n√£o vazios): 1,216,560  
Arquivo processado salvo em                : trn_processed.json  
```
---

# 3. Foundation Model (Modelo Base)

### **Modelo Escolhido:**

> **`unsloth/llama-3-8b-bnb-4bit`** - Implementa√ß√£o otimizada do LLaMA com:

- **Quantiza√ß√£o em 4 bits**
  - A quantiza√ß√£o em 4 bits reduz o consumo de mem√≥ria do modelo ao armazenar pesos de redes neurais com menor precis√£o.
  - Modelos tradicionais armazenam pesos com 32 bits (FP32) ou 16 bits (FP16), consumindo mais mem√≥ria.
  - 4-bit quantization representa cada peso com apenas 4 bits, reduzindo drasticamente o uso de mem√≥ria e permitindo a execu√ß√£o do modelo em GPUs com menos VRAM.
  - Isso melhora a efici√™ncia computacional, mas pode resultar em leve perda de precis√£o
- **Compat√≠vel com Hugging Face Transformers**
  - O modelo pode ser carregado e utilizado diretamente com a biblioteca Hugging Face Transformers, que √© um framework popular para LLMs.
- **Projetado para GPUs com CUDA** : CUDA (Compute Unified Device Architecture) √© a plataforma de computa√ß√£o paralela da NVIDIA, permitindo a execu√ß√£o eficiente de redes neurais em GPUs.
  - Otimiza√ß√£o para CUDA: O modelo aproveita opera√ß√µes aceleradas por GPU, como Flash Attention, que melhora a efici√™ncia da mem√≥ria.
  - Desempenho: Permite rodar infer√™ncias e treinamentos muito mais r√°pido do que em CPUs.
  - Compatibilidade: Suporta GPUs com Tensor Cores (ex: RTX 30xx, 40xx).
- **8 bilh√µes de par√¢metros**

**Caracter√≠sticas:**

- Otimizado para efici√™ncia em mem√≥ria e desempenho
- Suporta Flash Attention e LoRA
- Ideal para fine-tuning em recursos limitados
- Adequado para RAG (Retrieval Augmented Generation)
    
üìå **Fonte:** [Hugging Face Model Card](https://huggingface.co/unsloth/llama-3-8b-bnb-4bit)

O modelo escolhido para realizar o fine tunning neste trabalho √© o 
`unsloth/llama-3-8b-bnb-4bit`. Trata-se de uma implementa√ß√£o otimizada do modelo 
LLaMA (Large Language Model Meta AI), ajustada para ser eficiente em termos de 
mem√≥ria e desempenho. Ele utiliza quantiza√ß√£o em 4 bits, o que reduz 
significativamente o consumo de mem√≥ria sem comprometer a precis√£o do modelo. 
Al√©m disso, o modelo √© compat√≠vel com a biblioteca Hugging Face Transformers e 
foi projetado para ser executado em GPUs com suporte a CUDA, como a NVIDIA 
RTX 4060, aproveitando tecnologias como Flash Attention e LoRA (Low-Rank 
Adaptation) para acelerar o treinamento e a infer√™ncia. Com um tamanho de 8 
bilh√µes de par√¢metros, ele √© capaz de lidar com tarefas complexas de 
processamento de linguagem natural, como gera√ß√£o de texto, resumo, tradu√ß√£o e 
respostas a perguntas. 

O objetivo principal do modelo √© permitir o fine-tuning eficiente em cen√°rios 
com recursos computacionais limitados, como laptops ou esta√ß√µes de trabalho com 
GPUs de m√©dio porte. Ele foi otimizado para tarefas que exigem personaliza√ß√£o, 
como a adapta√ß√£o a dom√≠nios espec√≠ficos ou a cria√ß√£o de sistemas de recupera√ß√£o 
de informa√ß√µes baseados em RAG (Retrieval Augmented Generation). A combina√ß√£o de 
quantiza√ß√£o em 4 bits e adaptadores LoRA permite que o modelo seja ajustado 
rapidamente com conjuntos de dados menores, mantendo alta qualidade nas respostas 
geradas e reduzindo o tempo e os custos associados ao treinamento de modelos grandes.

# Exemplo de execu√ß√£o:

```
(.venv) PS C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama> python .\foundation_model.py
ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
ü¶• Unsloth Zoo will now patch everything to make training faster!

###############################################################################
# Criando Configura√ß√µes do Unsloth
###############################################################################

Configura√ß√£o realizada:
{'attn_implementation': 'flash_attention_2',
 'device_map': 'auto',
 'dtype': torch.bfloat16,
 'load_in_4bit': True,
 'lora_model': 'lora_model_llama-3-8b-bnb-4bit',
 'max_seq_length': 8192,
 'model': 'unsloth/llama-3-8b-bnb-4bit'}
==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.
   \\   /|    GPU: NVIDIA GeForce RTX 4060 Laptop GPU. Max memory: 7.996 GB. Platform: Windows.
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\.venv\Lib\site-packages\unsloth\models\llama.py:1185: 
UserWarning: expandable_segments not supported on this platform (Triggered internally at 
C:\actions-runner\_work\pytorch\pytorch\pytorch\c10/cuda/CUDAAllocatorConfig.h:28.)
  self.register_buffer("cos_cached", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)

Produto: Girls Ballet Tutu Neon Blue
Descri√ß√£o: This is a beautiful tutu. It is made of high quality material and it 
is very comfortable. It is perfect for any occasion and it is very affordable. 
It is also very durable and it will last for a long time. It is also very easy 
to maintain and it is very easy to clean. It is also very easy to wear and it 
is very easy to take off. It is also very easy to put on and it is very easy to 
take off. It is also very easy to put on and it is very easy to take off. It is 
also very easy to put on and it is very easy to take off. It is also very easy 
to put on and it is very easy to take off. It is also very easy to put on and it 
is very easy to take off. It is also very easy to put on and it is very easy to 
take off. It is also very easy to put on and it is very easy to take off. It is 
also very easy to put on and it is very easy to take off. It is also very easy 
to put on and it is very easy to take off. It is also very easy to put on and it 
is very easy to take off. It is also very easy to put on and it is very easy to 
take off. It is also very easy to put on and it is very easy to take off. It is 
also very easy to put on and it is very easy to take off. 

Produto: Mog's Kittens
Descri√ß√£o: The product is a pair of socks.

Produto: The Prophet
Descri√ß√£o: The Prophet is a product that helps you to find the best way to spend 
your money. It is a product that helps you to find the best way to spend your 
money. It is a product that helps you to find the best way to spend your money. 
It is a product that helps you to find the best way to spend your money. It is a 
product that helps you to find the best way to spend your money. It is a product 
that helps you to find the best way to spend your money. It is a product that 
helps you to find the best way to spend your money. It is a product that helps 
you to find the best way to spend your money. It is a product that helps you to 
find the best way to spend your money. It is a product that helps you to find 
the best way to spend your money. It is a product that helps you to find the 
best way to spend your money. 

Produto: The Book of Revelation
Descri√ß√£o: The Book of Revelation is a book of the New Testament of the Bible, 
and its title originated from the first word of the text in the Koine Greek: 
apokalypsis, meaning "unveiling" or "revelation". The author describes himself 
as "John" and does not identify himself as the son of Zebedee, the apostle John. 
The text is a letter to seven churches in the Roman province of Asia, and is a 
call to the churches to remain faithful to Jesus Christ, and individual letters 
to each church, with a promise of a swift punishment for Christian communities 
that are in a state of apostasy. The Book of Revelation is the final book of the 
New Testament and occupies a central place in Christian eschatology. By tradition, 
this prophecy was revealed by its author to the apostle John on the island of 
Patmos, and from its first readers, this prophecy has been accepted as of divine 
inspiration. The author of Revelation does not identify himself, but introduces 
his work as "the revelation of Jesus Christ", which he received "by an angel" from God. 

Process finished with exit code 0
```
---

# 4. Fine-Tuning do Modelo

## Processo em Duas Etapas
1. **Treinamento do modelo base**
2. **Interroga√ß√£o do modelo treinado**

O processo de fine-tuning do modelo envolve duas etapas: na primeira, realizamos o fine-tuning do modelo escolhido `("unsloth/llama-3-8b-bnb-4bit")` enquanto na segunda, interrogamos o modelo treinado `(./lora_model_llama-3-8b-bnb-4bit)`.

## Treinamento do Modelo

Nosso c√≥digo para o treinamento do modelo est√° dispon√≠vel no arquivo 
`fine_tuning.py`.

O proceso de fine-tuning √© feito a partir do arquivo processado 
`trn_processed.json` na fase de prepara√ß√£o dos dados para treinamento.

**Principais Componentes:**
  - Uso de Unsloth e LoRA para efici√™ncia
  - Formata√ß√£o Alpaca para prompts
  - Redu√ß√£o para 5.000 itens (devido a restri√ß√µes de tempo)

O uso do Unsloth e do LoRA foi motivado pela necessidade 
de realizar o fine-tuning de modelos grandes de forma eficiente e com menor 
consumo de recursos computacionais. O Unsloth oferece otimiza√ß√µes espec√≠ficas 
para acelerar o treinamento, enquanto o LoRA permite ajustar apenas um pequeno 
n√∫mero de par√¢metros, reduzindo significativamente os requisitos de mem√≥ria e 
tempo de execu√ß√£o, sem comprometer a qualidade do modelo treinado.

Inicialmente, a configura√ß√£o do Unsloth √© feita de forma a preparar o modelo 
para fine-tuning. Com a configura√ß√£o pronta, modelo e tokenizador s√£o carregados
e retornado. Depois, aplicamos os adaptadores LoRA ao mesmo modelo, Deixando-o 
pronto para o treinamento.

Neste momento, iniciamos a prepara√ß√£o dos dados que ser√£o utilizados para o 
fine-tuning do modelo. Ele √© preparado de forma a agrupar os produtos como 
grupo de entrada (`input`) e as descri√ß√µes como grupo de sa√≠da (`output`). As 
instru√ß√µes (perguntas que ser√£o feitas ao modelo) ficam no grupo de 
instru√ß√µes (`instructions`). Esta prepara√ß√£o √© salva em arquivo 
(`trn_processed_dataset.json`) para ser utilizada mais adiante.

Al√©m disso, o modelo foi ajustado utilizando o Alpaca, que aprimora a capacidade 
do modelo em lidar com consultas complexas, garantindo respostas mais precisas e 
relevantes, aumentando a efici√™ncia em recupera√ß√£o de informa√ß√µes e gera√ß√£o de 
textos.

Ap√≥s ser carregado como um dataset, √© transformado em um "prompt dataset"
para ser passado para o treinador do modelo. Ent√£o, o treinamento √© realizado.
Neste momento, uma mudan√ßa de foco precisou ser feita: a quantidade de itens 
a serem treinadas (1.216.560) implicava em um tempo de treinamento que excediam 
7 dias. Com isto, o "prompt dataset" foi reduzido para 5000 item, o que permitiu
a aplica√ß√£o de duas √©pocas de treinamento (algo em torno de 1250 itera√ß√µes) 
tomando um total 2:30h para que a tarefa fosse realizada. √â importante ressaltar 
que a configura√ß√£o presente no c√≥digo, a GPU utilizada e o conjunto de dados de 
treino influenciam diretamente no tempo de execu√ß√£o do treinamento. Algumas 
estat√≠sticas s√£o mostradas ap√≥s o treino (tempo gasto, consumo de mem√≥ria).

Ao finalizarmos esta etapa, preparamos o modelo treinado para a realiza√ß√£o de 
infer√™ncias. Duas consultas s√£o realizadas, apenas para mostrar as capacidades ### ARRUMAR
de exibi√ß√£o das respostas √†s consultas: a exibi√ß√£o completa, ap√≥s o resultado 
retornado ou sua apresenta√ß√£o conforme o modelo vai gerando a resposta.

Conclu√≠do este pequeno teste, salvamos o modelo e seus adaptadores LoRA 
localmente.

O dataset foi carregado e processado de forma a ser compat√≠vel com o formato 
esperado pela biblioteca Hugging Face. Isso permite que ele seja utilizado 
diretamente em pipelines de treinamento e infer√™ncia, facilitando a integra√ß√£o 
com modelos de aprendizado de m√°quina e otimizando o fluxo de trabalho.

## **Par√¢metros de Treinamento:**

### **üîπ Tamanho dos Batches e Gradientes**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `per_device_train_batch_size = 2` | Define o n√∫mero de exemplos processados por batch em cada GPU. Um batch pequeno consome menos mem√≥ria, mas pode afetar a estabilidade do treinamento. |
| `gradient_accumulation_steps = 4` | Acumula gradientes por 4 passos antes de atualizar os pesos do modelo. Isso simula um batch maior sem exigir mais mem√≥ria da GPU. |

**Exemplo:** Se `batch_size = 2` e `gradient_accumulation_steps = 4`, o modelo s√≥ atualiza os pesos ap√≥s processar **8 exemplos**.

---

### **üîπ Etapas de Treinamento**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `warmup_steps = 5` | N√∫mero de passos iniciais onde a taxa de aprendizado cresce gradualmente para evitar varia√ß√µes bruscas no gradiente. |
| `max_steps = 60` | N√∫mero total de passos de treinamento. Neste caso, √© um teste. Para um treinamento real, pode-se definir `num_train_epochs`. |
| `#num_train_epochs = 2` | Define quantas √©pocas completas o dataset ser√° percorrido durante o treinamento. |

---

### **üîπ Taxa de Aprendizado e Otimiza√ß√£o**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `learning_rate = 2e-4` | Define a taxa de aprendizado do otimizador. Valores altos aceleram o aprendizado, mas podem ser inst√°veis. |
| `weight_decay = 0.01` | Regulariza√ß√£o L2 para evitar overfitting, penalizando pesos muito grandes. |
| `lr_scheduler_type = "linear"` | Define o decaimento da taxa de aprendizado. O tipo `linear` reduz a taxa gradualmente at√© o final do treinamento. |

---

### **üîπ Precis√£o e Performance**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `fp16 = not is_bfloat16_supported()` | Usa **FP16** (16-bit floating point) se `bfloat16` n√£o estiver dispon√≠vel. FP16 economiza mem√≥ria, mas pode ser inst√°vel. |
| `bf16 = is_bfloat16_supported()` | Usa **bfloat16** se a GPU suportar. BF16 √© mais est√°vel que FP16, consumindo a mesma quantidade de mem√≥ria. |
| `gradient_checkpointing = True` | Ativa **gradient checkpointing**, salvando menos ativa√ß√µes durante o forward pass para economizar VRAM. Isso reduz o consumo de mem√≥ria, mas aumenta o tempo de treinamento. |
| `optim = "adamw_8bit"` | Usa o otimizador **AdamW** em 8 bits, reduzindo o uso de mem√≥ria do otimizador sem perder efici√™ncia. |
| `max_grad_norm = 0.3` | Limita o valor m√°ximo dos gradientes para evitar explos√µes no treinamento. |

---

### **üîπ Logging e Salvamento**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `logging_steps = 1` | Define a frequ√™ncia com que m√©tricas como **loss** s√£o registradas. Valores menores geram logs mais frequentes. |
| `output_dir = "trainer_outputs"` | Define o diret√≥rio onde os logs e checkpoints do modelo ser√£o salvos. |
| `save_steps = 1000` | Frequ√™ncia com que o modelo √© salvo durante o treinamento. Um valor muito baixo pode gerar arquivos desnecess√°rios e ocupar espa√ßo. |

---

### **üîπ Reprodutibilidade**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `seed = 3407` | Define uma semente fixa para garantir que os experimentos sejam reproduz√≠veis. Isso significa que, ao rodar o treinamento novamente, os resultados ser√£o os mesmos. |

---

## **SFTTrainer**

O `SFTTrainer` (Supervised Fine-Tuning Trainer) √© uma classe especializada para **fine-tuning eficiente** usando LoRA. Ele recebe os argumentos definidos acima (`args = training_arguments`) e adiciona configura√ß√µes espec√≠ficas.

### **üîπ Configura√ß√µes B√°sicas**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `model = peft_model` | O modelo que ser√° treinado. Neste caso, um modelo **LoRA ajustado**. |
| `tokenizer = tokenizer` | O tokenizador usado para processar os textos antes do treinamento. |
| `train_dataset = prompt_dataset` | O dataset formatado no padr√£o necess√°rio para o treinamento. |
| `dataset_text_field = "text"` | Define qual campo do dataset cont√©m o texto a ser usado no treinamento. |

---

### **üîπ Tamanho da Sequ√™ncia e Processamento**

| Argumento | Descri√ß√£o |
|-----------|------------|
| `max_seq_length = 8192` | Define o tamanho m√°ximo de tokens que o modelo pode processar em uma √∫nica entrada. |
| `dataset_num_proc = 1` | N√∫mero de processos paralelos para pr√©-processamento do dataset. Valores maiores podem acelerar, mas exigem mais CPU. |
| `packing = False` | Define se entradas curtas devem ser concatenadas para otimizar o uso de espa√ßo. No caso, est√° desativado. |

---

# Explica√ß√£o dos Par√¢metros de Gera√ß√£o de Texto

A fun√ß√£o `.generate()` do modelo ajustado (`peft_model`) √© respons√°vel por gerar texto com base em um input processado. Cada argumento influencia diretamente **a forma como o modelo gera texto**, afetando **comprimento, aleatoriedade e efici√™ncia**.

```python
outputs = peft_model.generate(
    **inputs,
    max_new_tokens = 128,  # M√°ximo de tokens na resposta
    temperature    = 0.2,  # Controla aleatoriedade (0.0 a 1.0)
    do_sample      = True, # Usa amostragem probabil√≠stica
    use_cache      = True  # Ativa cache para melhorar a velocidade de gera√ß√£o
)
```

| Par√¢metro | O que faz? | Valores recomendados |
|-----------|-----------|---------------------|
| **`max_new_tokens`** | Define **o n√∫mero m√°ximo de novos tokens** que podem ser gerados **na resposta** | `50-200` (depende do contexto) |
| **`temperature`** | Controla o n√≠vel de **aleatoriedade** da gera√ß√£o de texto | `0.3` (formal) - `0.8` (criativo) |
| **`do_sample`** | Ativa **amostragem probabil√≠stica** | `False` (respostas fixas) - `True` (criatividade) |
| **`use_cache`** | Usa cache para **acelerar gera√ß√£o** | Sempre `True` |

**üîπExemplo "max_new_tokens":** Se `max_new_tokens = 128`, o modelo pode **gerar at√© 128 tokens** depois do prompt de entrada.
  - Um valor muito **baixo** pode truncar a resposta antes que ela seja conclu√≠da.
  - Um valor muito **alto** pode gerar respostas longas e desnecess√°rias, consumindo mais mem√≥ria e tempo de infer√™ncia

**üîπExemplo "temperature":** Escolhemos utilizar uma temperatura de `0.2` pois queriamos algo mais fixo, visto que deve ter um comportamento mais inclinado ao `RAG`.
  - `temperature = 0.0` ‚Üí **Texto mais determin√≠stico** (o modelo escolhe sempre o token com maior probabilidade).
  - `temperature = 1.0` ‚Üí **Texto mais criativo e diversificado** (o modelo escolhe tokens menos prov√°veis com mais frequ√™ncia).

**üîπExemplo "do_sample":** Permite que o modelo **n√£o escolha sempre o token mais prov√°vel**.
  - Se `do_sample = False`, o modelo **sempre escolhe o token com maior probabilidade**, tornando as respostas **muito previs√≠veis**.
  - Se `do_sample = True`, o modelo **pode escolher tokens menos prov√°veis**, tornando o texto mais **variado e criativo**.

**üîπExemplo "use_cache":** Ativa um **cache interno** para acelerar a gera√ß√£o de tokens.
  - Durante a gera√ß√£o, o modelo precisa **calcular os tokens anteriores repetidamente**.
  - Com **`use_cache = True`**, ele **armazena os tokens j√° processados**, evitando recomputa√ß√£o desnecess√°ria.

--- 

### **Estat√≠sticas de Treinamento:**

```
Tempo total de treinamento: 49274.93 segundos (821.25 min)
Velocidade de processamento: 0.20 samples/s
Loss final: 1.8543
Mem√≥ria GPU utilizada: 9.125 GB (114.12% da capacidade)
```
üìå **Modelo treinado dispon√≠vel em:** [ACMattosHE/lora_model_llama-3-8b-bnb-4bit](https://huggingface.co/ACMattosHE/lora_model_llama-3-8b-bnb-4bit/tree/main)

## Exemplo de execu√ß√£o:
A seguir, o log de execu√ß√£o deste c√≥digo:

``` 
(.venv) PS C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama> python .\fine_tuning.py  
ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.  
ü¶• Unsloth Zoo will now patch everything to make training faster!  

###############################################################################  
# Informa√ß√µes do Ambiente de Execu√ß√£o  
###############################################################################  
  
Vers√£o do Python      : 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]  
Vers√£o do PyTorch     : 2.6.0+cu126  
Vers√£o do Triton      : 3.2.0  
Vers√£o do Transformers: 4.49.0  
CUDA dispon√≠vel       : True  
Vers√£o do CUDA        : 12.6  
Vers√£o do cuDNN       : 90501  
N√∫meros de GPUs       : 1  
Nome da GPU 0         : NVIDIA GeForce RTX 4060 Laptop GPU  
Mem√≥ria M√°xima        : 7.996 GB  
Mem√≥ria reservada     : 0.0 GB  

###############################################################################  
# Criando Configura√ß√µes do Unsloth  
###############################################################################  
  
Configura√ß√£o realizada:  
{'attn_implementation': 'flash_attention_2',  
 'device_map': 'auto',  
 'dtype': torch.bfloat16,  
 'load_in_4bit': True,  
 'lora_model': 'lora_model_llama-3-8b-bnb-4bit',  
 'max_seq_length': 8192,  
 'model': 'unsloth/llama-3-8b-bnb-4bit'}  

###############################################################################  
# Carregando o modelo e tokenizador para o modelo...  
###############################################################################  
  
==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.  
   \\   /|    GPU: NVIDIA GeForce RTX 4060 Laptop GPU. Max memory: 7.996 GB. Platform: Windows.  
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0  
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]  
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth  
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!  
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\.venv\Lib\site-packages\unsloth\models\llama.py:1185: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\c10/cuda/CUDAAllocatorConfig.h:28.)  
  self.register_buffer("cos_cached", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)  
  
Modelo unsloth/llama-3-8b-bnb-4bit e tokenizador carregados e com sucesso!  
  
###############################################################################  
# Configurando o modelo para fine-tuning com LoRA (Low Rank Adaptation)...  
###############################################################################  
  
Unsloth 2025.1.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.  
  
Modelo configurado para fine-tuning com LoRA (Low Rank Adaptation)  
  
###############################################################################  
# Criando o Conjunto de Dados para Treinamento do Modelo...  
###############################################################################  
  
Processando linhas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5002/5002 [00:00<00:00, 277916.39it/s]   
  
###############################################################################  
# Gerando o Arquivo com o Conjunto de Dados para Treinamento do Modelo...  
###############################################################################  
  
Salvando registros: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 5001554.97it/s]  
  
Dataset salvo em                             : trn_processed_dataset.json  
  
###############################################################################  
# Gerando o Dataset para Treinamento do Modelo...  
###############################################################################  
  
Generating train split: 5000 examples [00:00, 59244.76 examples/s]  
  
Dataset geardo com sucesso: Dataset({  
    features: ['instruction', 'input', 'output'],  
    num_rows: 5000  
})  
  
###############################################################################  
# Convertendo o Dataset para Treinamento do Modelo em Prompts...  
###############################################################################  
  
Formatando prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 70249.52 examples/s]  
  
Dataset convertido com sucesso: Dataset({  
    features: ['instruction', 'input', 'output', 'text'],  
    num_rows: 5000  
})  
  
###############################################################################  
# Configurando o Treinador SFT (Supervised Fine-Tuning)...  
###############################################################################  
  
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 6432.07 examples/s]  
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` 
hides base models input arguments, if label_names is not given, label_names can't 
be set automatically within `Trainer`. Note that empty label_names list will be 
used instead.       
  
Treinador SFT configurado com sucesso!  
  
###############################################################################  
# Treinando o Treinador SFT (Supervised Fine-Tuning)...  
###############################################################################  
  
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1  
   \\   /|    Num examples = 5,000 | Num Epochs = 1  
O^O/ \_/ \    Batch size per device = 2 | Gradient Accumulation steps = 4  
\        /    Total batch size = 8 | Total steps = 1,250  
 "-____-"     Number of trainable parameters = 83,886,080  
{'loss': 3.2363, 'grad_norm': 1.2117664813995361, 'learning_rate': 0.00019984, 'epoch': 0.0}
{'loss': 3.1007, 'grad_norm': 0.7599466443061829, 'learning_rate': 0.00019968, 'epoch': 0.0}
{'loss': 2.8482, 'grad_norm': 1.2890647649765015, 'learning_rate': 0.00019952000000000001, 'epoch': 0.0}
{'loss': 2.6395, 'grad_norm': 0.8345205187797546, 'learning_rate': 0.00019936000000000002, 'epoch': 0.01}
{'loss': 2.6946, 'grad_norm': 0.9719563722610474, 'learning_rate': 0.00019920000000000002, 'epoch': 0.01}
{'loss': 2.6354, 'grad_norm': 1.515956997871399, 'learning_rate': 0.00019904, 'epoch': 0.01} 
{'loss': 2.4815, 'grad_norm': 0.8237305283546448, 'learning_rate': 0.00019888, 'epoch': 0.01}
{'loss': 2.6736, 'grad_norm': 0.7439571022987366, 'learning_rate': 0.00019872000000000002, 'epoch': 0.01}
{'loss': 2.415, 'grad_norm': 0.7331687211990356, 'learning_rate': 0.00019856000000000002, 'epoch': 0.01}
{'loss': 2.3698, 'grad_norm': 0.9583930373191833, 'learning_rate': 0.0001984, 'epoch': 0.02}
{'loss': 1.9416, 'grad_norm': 1.1305814981460571, 'learning_rate': 0.00019824, 'epoch': 0.02}
{'loss': 2.1291, 'grad_norm': 1.077892780303955, 'learning_rate': 0.00019808, 'epoch': 0.02} 
{'loss': 2.4154, 'grad_norm': 3.3878471851348877, 'learning_rate': 0.00019792000000000003, 'epoch': 0.02}
{'loss': 1.9818, 'grad_norm': 0.8834867477416992, 'learning_rate': 0.00019776, 'epoch': 0.02}
{'loss': 2.082, 'grad_norm': 1.0258870124816895, 'learning_rate': 0.0001976, 'epoch': 0.02}
{'loss': 2.0796, 'grad_norm': 0.9682966470718384, 'learning_rate': 0.00019744, 'epoch': 0.03}
{'loss': 2.1334, 'grad_norm': 0.7916772961616516, 'learning_rate': 0.00019728, 'epoch': 0.03}
{'loss': 2.1901, 'grad_norm': 0.6517899036407471, 'learning_rate': 0.00019712, 'epoch': 0.03}
{'loss': 2.0756, 'grad_norm': 0.8336527347564697, 'learning_rate': 0.00019696, 'epoch': 0.03} 
...........
{'loss': 1.6027, 'grad_norm': 0.6916661858558655, 'learning_rate': 1.44e-06, 'epoch': 1.99}
{'loss': 1.164, 'grad_norm': 0.7378243207931519, 'learning_rate': 1.28e-06, 'epoch': 1.99}
{'loss': 1.5617, 'grad_norm': 0.870815098285675, 'learning_rate': 1.12e-06, 'epoch': 1.99}
{'loss': 1.8097, 'grad_norm': 0.7309754490852356, 'learning_rate': 9.6e-07, 'epoch': 1.99}
{'loss': 1.8802, 'grad_norm': 0.7114237546920776, 'learning_rate': 8.000000000000001e-07, 'epoch': 1.99}
{'loss': 1.945, 'grad_norm': 0.8869221806526184, 'learning_rate': 6.4e-07, 'epoch': 1.99}
{'loss': 1.7192, 'grad_norm': 0.7447224855422974, 'learning_rate': 4.8e-07, 'epoch': 2.0}
{'loss': 1.5252, 'grad_norm': 0.905275821685791, 'learning_rate': 3.2e-07, 'epoch': 2.0}
{'loss': 1.8738, 'grad_norm': 0.8697205185890198, 'learning_rate': 1.6e-07, 'epoch': 2.0}
{'loss': 1.6066, 'grad_norm': 0.8066918253898621, 'learning_rate': 0.0, 'epoch': 2.0}
{'train_runtime': 49274.9322, 'train_samples_per_second': 0.203, 'train_steps_per_second': 0.025, 'train_loss': 1.8542551296710967, 'epoch': 2.0}
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:41:14<00:00, 39.42s/it] 

Estat√≠sticas do Treinamento
===========================
Tempo total de treinamento       : 49274.93 segundos (821.25 min)
Velocidade de processamento      : 0.20 samples/s
Loss final do treinamento        : 1.8543

Estat√≠sticas de Mem√≥ria GPU
============================
Pico de mem√≥ria reservada        : 9.125 GB
Pico de mem√≥ria para treinamento : 0.000 GB
Percentual da mem√≥ria m√°xima     : 114.120%
Percentual usado no treinamento  : 0.000%
  
AVISO: O uso de mem√≥ria ultrapassou o limite m√°ximo recomendado!  
  
Treinamento concluido com sucesso!  
  
###############################################################################  
# Preparando o modelo para fazer infer√™ncia (previs√µes)...  
###############################################################################  
  
Modelo preparado para infer√™ncia!  
  
###############################################################################  
# Preparando 'Mog's Kittens' para ser tokenizado e passar por infer√™ncia...  
###############################################################################  
  
Tokeniza√ß√£o realizada com sucesso!  
  
###############################################################################  
# Realizando pergunta para o modelo...  
###############################################################################  
  
Resposta obtida com sucesso!  
  
Resposta do modelo: [The New York Times bestselling author of the beloved Mog 
series returns with a new tale of a cat and her kittens  Mog is a cat who loves 
to sleep in the sun and eat tuna fish But when she has kittens of her own she must 
learn to be a good mother and teach her kittens to be good cats too  This is a 
sweet and funny story about a cat and her kittens that is sure to delight young 
readers  Ages 3 to 7]  
  
###############################################################################  
# Preparando 'Mog's Kittens' para ser tokenizado e passar por infer√™ncia...  
###############################################################################  
  
Tokeniza√ß√£o realizada com sucesso!  

 The New York Times bestselling author of the beloved Mog series returns with a 
 new story about the little cat who has charmed millions of readers worldwideMog 
 is a cat who likes to be in charge of things Mog likes to be in charge of the 
 other cats Mog likes to be
 in charge of the dog Mog likes to be in charge of the fish Mog likes to be in
  charge of the bird Mog likes to be in charge of the mouse Mog likes to be in 
  charge of the baby Mog likes to be in charge of the worldBut when the baby is
   born Mog finds that being in charge is not as easy as she thought it would beM  
  
Resposta obtida com sucesso!  
  
###############################################################################  
# Salvando o Modelo Treinado em lora_model_llama-3-8b-bnb-4bit...  
###############################################################################  
Unsloth: Merging 4bit and LoRA weights to 16bit...  
Unsloth: Will use up to 0.0 out of 31.73 RAM for saving.  
Unsloth: Saving model... This might take 5 minutes ...  
  0%|                                                    | 0/32 [00:00<?, ?it/s]   
We will save to Disk and not RAM now.  
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [01:31<00:00,  2.85s/it]  
Unsloth: Saving tokenizer... Done.  
Done.  
  
>>>>>>>>>>>>>>>>>>>>>>>>>>>>> FIM DO FINE-TUNING <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<  
```

## Testando Modelo Treinado

Nosso c√≥digo para o treinamento do modelo est√° dispon√≠vel no arquivo 
`"ft_test_trained_model.py"`.

O proceso de fine-tuning √© feito a partir do arquivo processado 
(`trn_processed.json`) na fase de prepara√ß√£o dos dados para treinamento.

## Exemplo de execu√ß√£o:

```
(.venv) PS C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama> python .\ft_test_trained_model.py  
ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.  
ü¶• Unsloth Zoo will now patch everything to make training faster!  
  
###############################################################################  
# Criando Configura√ß√µes do Unsloth  
###############################################################################  
  
Configura√ß√£o realizada:  
{'attn_implementation': 'flash_attention_2',  
 'device_map': 'auto',  
 'dtype': torch.bfloat16,  
 'load_in_4bit': True,  
 'lora_model': 'lora_model_llama-3-8b-bnb-4bit',  
 'max_seq_length': 8192,  
 'model': 'unsloth/llama-3-8b-bnb-4bit'}  
  
###############################################################################  
# Perguntando ao modelo treinado: ./lora_model_llama-3-8b-bnb-4bit  
###############################################################################  
  
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be 
removed in the future versions. Please, pass a `BitsAndBytesConfig` object in 
`quantization_config` argument instead.  
`low_cpu_mem_usage` was None, now default to True since model is quantized.  
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\.venv\Lib\site-packages\accelerate\utils\modeling.py:330: 
UserWarning: expandable_segments not supported on this platform (Triggered internally at 
C:\actions-runner\_work\pytorch\pytorch\pytorch\c10/cuda/CUDAAllocatorConfig.h:28.)
  new_value = value.to(device)  
  
Query:  Girls Ballet Tutu Neon Blue  
  
Resposta do modelo:  
 The perfect gift for the little ballerina in your life this tutu is made of soft 
 nylon tulle and is fully lined with a satin drawstring waistband<|end_of_text|>  
  
Query:  Mog's Kittens  
  
Resposta do modelo:  
 Praise for Mog the Forgetful Catx2018Grandparents are likely to get as much fun 
 out of seeing it again as the new generation of fans just learning to readx2019 
 Choice Magazinex2018A lovely book for all Mogfanciersx2019 The 
 Observerx2018Kerrx2019s watercolours are
 full of humour and expressionx2019 Financial Timesx2018A lovely book for all 
 Mogfanciersx2019 The Observerx2018Kerrx2019s watercolours are full of humour and 
 expressionx2019 Financial Times<|end_of_text|>  
  
Query:  The Prophet  
  
Resposta do modelo:  
 The Prophet is a book of 26 prose poetry essays written in English by 
 LebaneseAmerican artist Kahlil Gibran 1883ndash1931 The Prophet has been 
 translated into over 20 languages and has sold more than 100 million copies 
 worldwide<|end_of_text|>  
  
Query:  The Book of Revelation  
  
Resposta do modelo:  
 The Book of Revelation is the last book of the New Testament and one of the 
 most enigmatic and controversial works in Western literature It is a book of 
 apocalyptic prophecy that predicts the end of the world and the Last Judgment 
 The book is also known as the Apocalypse of John or simply the Apocalypse<|end_of_text|>  
``` 

# 5. Implementa√ß√£o de RAG (Retrieval Augmented Generation)

### **Processo em Duas Etapas:**
1. **Indexa√ß√£o dos dados** no ChromaDB
2. **Consulta do modelo com base indexada**

O processo de RAG (Retrieval Augmented Generation) do modelo envolve duas 
etapas: na primeira, realizamos a indexa√ß√£o dos dados que iremos trabalhar
com o modelo de IA escolhido (`unsloth/llama-3-8b-bnb-4bit`) enquanto na 
segunda, interrogamos o modelo utilizando a base de dados indexada, utilizando RAG.
Uma etapa intermedi√°ria foi introduzida, apenas para verificar a indexa√ß√£o dos 
dados de trabalho `(trn_processed.json)`. 

## Indexa√ß√£o dos Dados

Nosso c√≥digo para a indexa√ß√£o dos dados para realiza√ß√£o e RAG est√° dispon√≠vel no
arquivo `rag_indexing.py`.

## Fluxo:

  1. Carregamento de trn_processed.json
  2. Quebra em chunks menores
  3. Gera√ß√£o de embeddings
  4. Armazenamento no ChromaDB

O in√≠cio do processo se d√° com o consumo do arquivo `trn_processed.json`. Ele √© 
carregado para que seus dados sejam preparados para a indexa√ß√£o na vector store
(`ChromaDB`). Para otimizar as busca, os dados s√£o quebrados em peda√ßos menores 
(chunks), antes de serem convertidos em vetores de embeddings. Com os embeddings
gerados, chegou o momento de converter os dados processados em documentos que 
ser√£o armazenados na vector store. Tendo os documentos prontamente convertidos, 
chegou o momento de instanciar a vector store e ent√£o armazenar documentos e 
embeddings criados.  

## Exemplo de execu√ß√£o:

```
(.venv) PS C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama> python .\rag_indexing.py 

Carregando dados do arquivo processado...
Total de registros gerados:5000
Exemplo de registro: 
{'description': 'High quality 3 layer ballet tutu 12 inches in length',
 'product': 'Girls Ballet Tutu Neon Pink'}

Quebrando documentos JSON em chunks...

Total de chunks gerados: 5000

Exemplo de chunk:
Product: Girls Ballet Tutu Neon Pink - Description: High quality 3 layer ballet tutu 12 inches in length

Criando modelo de embeddings...
Dividindo documentos JSON: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 555316.30it/s]
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama\rag_indexing.py:68: 
LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in 
LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists 
in the :class:`~langchain-huggingface package and should be used instead. To use 
it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:
`~langchain_huggingface import HuggingFaceEmbeddings``.
  embeddings_model = HuggingFaceEmbeddings(

Criando modelo de embeddings...

Gerando embeddings para os chunks...

Gerando embeddings...

Total de embeddings gerados: 5000

Exemplo de embedding:
[0.03302772715687752,
 -0.021298706531524658,
 -0.021273870021104813,
 0.052845340222120285,
 -0.019837183877825737,
 -0.03264341503381729,
 0.010283859446644783,
 0.014730839990079403,
 -0.017746785655617714,
 0.003939006011933088,
  6.719978057498912e-35,
 0.037213534116744995,
 -0.10796977579593658,
 0.04069541022181511,
 -0.002336055040359497,
 0.006895443890243769,
 -0.05344033241271973,
 0.08863085508346558,
 -0.03707418590784073,
 0.05612768977880478,
 0.00930757075548172,
 -0.023683171719312668]

Convertendo json_data em lc_documents...

C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama\rag_indexing.py:122: 
LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 
and will be removed in 1.0. An updated version of the class exists in the :class:
`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:
`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.
  vector_store = Chroma(

Total de lc_documents  gerados: 5000

Exemplo de lc_document:
Document(metadata={'product': 'Girls Ballet Tutu Neon Pink'}, 
page_content='Product: Girls Ballet Tutu Neon Pink - Description: High quality 
3 layer ballet tutu 12 inches in length')

Criando vector_store para o embeddings_model...

vector_store criada!

Populando vector_store...

Gerando UUIDs para cada documento...

Adicionando os documentos no vector_store...

Documentos carregados!
```

## Verifica√ß√£o dos Dados Indexados (Passo Intermedi√°rio)
Nosso c√≥digo para a verifica√ß√£o dos dados que foram indexados anteriormente est√°
dispon√≠vel no arquivo `rag_search_vs.py`.

Para realizar a verifica√ß√£o dos dados indexados, utilizamos o modelo de embeddings
criado na indexa√ß√£o dos dados. Em seguida, utilizamos a vector store para realizar
buscas e exibir os resultados.

Os resultados retornados s√£o o 3 mais relevantes ao produto pesquisado. O 
exemplo de execu√ß√£o abaixo mostra os resultados obtidos para o produto
`Girls Ballet Tutu Neon Pink`.  

## Exemplo de execu√ß√£o:

```
(.venv) PS C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama> python .\rag_search_vs.py  
  
Criando modelo de embeddings...  
  
Criando modelo de embeddings...  
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama\rag_indexing.py:68:   
LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in   
LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists   
in the :class:`~langchain-huggingface package and should be used instead. To use it run `  
pip install -U :class:`~langchain-huggingface` and import as   
`from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.  
  embeddings_model = HuggingFaceEmbeddings(  
  
Criando vector_store para o embeddings_model...  
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama\rag_indexing.py:122:   
LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9   
and will be removed in 1.0. An updated version of the class exists in the :class:  
`~langchain-chroma package and should be used instead. To use it run `  
pip install -U :class:`~langchain-chroma` and import as   
`from :class:`~langchain_chroma import Chroma``.  
  vector_store = Chroma(  
  
vector_store criada!  
  
Realizando consultas na vector_store  
  
* Product: Girls Ballet Tutu Neon Blue - Description: Dance tutu for girls ages 
  28 years Perfect for dance practice recitals and performances costumes or just 
  for fun [{'product': 'Girls Ballet Tutu Neon Blue'}]  

* Product: Girls Ballet Tutu Neon Pink - Description: High quality 3 layer ballet 
  tutu 12 inches in length [{'product': 'Girls Ballet Tutu Neon Pink'}]  

* Product: Delphie and the Birthday Show Magic Ballerina Book 6 - Description: 
  x201CDonx2019t be surprised if your child asks for a magical pair of red ballet 
  shoesx201D Telegraph Magazinex201CDelightfulx201D You Magazine Mail on 
  Sundayx201CA delight for any young reader who sees herself as a budding 
  ballerinax201D MumKnowsBestcouk [{'product': 'Delphie and the Birthday Show 
  Magic Ballerina Book 6'}]  
```

## Testando o Modelo com RAG

Nosso c√≥digo para a realiza√ß√£o de consultas ao modelo utilizando RAG est√° 
dispon√≠vel no arquivo `rag_model_retriever.py`.

## Exemplo de execu√ß√£o:

```
(.venv) PS C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama> python .\rag_model_retriever.py  
ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.  
ü¶• Unsloth Zoo will now patch everything to make training faster!  
  
Criando modelo de embeddings...  
  
Criando modelo de embeddings...  
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama\rag_indexing.py:68: 
LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in 
LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists 
in the :class:`~langchain-huggingface package and should be used instead. To use 
it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:
`~langchain_huggingface import HuggingFaceEmbeddings``.  
  embeddings_model = HuggingFaceEmbeddings(  
  
Criando vector_store para o embeddings_model...  
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama\rag_indexing.py:122: 
LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 
and will be removed in 1.0. An updated version of the class exists in the :class:
`~langchain-chroma package and should be used instead. To use it run `pip install 
-U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.  
  vector_store = Chroma(  
  
vector_store criada!  
  
###############################################################################  
# Criando Configura√ß√µes do Unsloth  
###############################################################################  
  
Configura√ß√£o realizada:  
{'attn_implementation': 'flash_attention_2',  
 'device_map': 'auto',  
 'dtype': torch.bfloat16,  
 'load_in_4bit': True,  
 'lora_model': 'lora_model_llama-3-8b-bnb-4bit',  
 'max_seq_length': 8192,  
 'model': 'unsloth/llama-3-8b-bnb-4bit'}  
  
###############################################################################  
# Carregando o modelo e tokenizador para o modelo...  
###############################################################################  
  
==((====))==  Unsloth 2025.1.5: Fast Llama patching. Transformers: 4.49.0.  
   \\   /|    GPU: NVIDIA GeForce RTX 4060 Laptop GPU. Max memory: 7.996 GB. Platform: Windows.  
O^O/ \_/ \    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0  
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]  
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth   
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!  
  
Modelo unsloth/llama-3-8b-bnb-4bit e tokenizador carregados e com sucesso!  
  
###############################################################################  
# Preparando o modelo para fazer infer√™ncia (previs√µes)...  
###############################################################################  
  
Modelo preparado para infer√™ncia!  
C:\acmattos\dev\tools\Python\ia4devs\module_03\04_tech_challenge\llama\rag_model_retriever.py:35: 
LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` 
was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:
`~invoke` instead.  
  retrieved_docs = retriever.get_relevant_documents(product)  
  
Resposta gerada:  
  
Below is an instruction that describes a task, paired with an input that   
provides further context. Write a response that appropriately completes   
the request.  
  
### Instruction:  
GET THE DESCRIPTION OF THIS PRODUCT: Girls Ballet Tutu Neon Blue  
  
### Input:  
Product: Girls Ballet Tutu Neon Blue - Description: Dance tutu for girls ages 28 
years Perfect for dance practice recitals and performances costumes or just for fun  
Product: Girls Ballet Tutu Neon Pink - Description: High quality 3 layer ballet 
tutu 12 inches in length  
Product: Delphie and the Birthday Show Magic Ballerina Book 6 - Description: 
x201CDonx2019t be surprised if your child asks for a magical pair of red ballet 
shoesx201D Telegraph Magazinex201CDelightfulx201D You Magazine Mail on Sundayx201CA 
delight for any young reader who sees herself as a budding ballerinax201D MumKnowsBestcouk  

### Response:  

The description of this product is: Girls Ballet Tutu Neon Blue - Description: 
Dance tutu for girls ages 28 years Perfect for dance practice recitals and 
performances costumes or just for fun  

Resposta gerada:  
  
Below is an instruction that describes a task, paired with an input that   
provides further context. Write a response that appropriately completes   
the request.  

### Instruction:  
GET THE DESCRIPTION OF THIS PRODUCT: Mog's Kittens  
  
### Input:  
Product: Mogs Kittens - Description: Judith Kerr8217s best8211selling adventures 
of that endearing and exasperating cat Mog have entertained children for more 
than 30 years Now even infants and toddlers can enjoy meeting this loveable 
feline These sturdy little board books8212with their bright simple pictures easy 
text and hand8211friendly formats8212are just the thing to delight the very young 
Ages 6 months82112 years  
Product: Mog and the VET Mog the Cat Books - Description: Praise for Mog the 
Forgetful Catx2018Grandparents are likely to get as much fun out of seeing it 
again as the new generation of fans just learning to readx2019 Choice Magazinex2018A 
lovely book for all Mogfanciersx2019 The ObserverPraise for Goodbye Mogx2018Kerrx2019s 
warmth humour and honesty make this an engaging introduction to a difficult topicx2019 
Financial Timesx2018Believable amusing and movingx2019 Nursery Worldx2018A supremely 
sensitive storyx2019 The Timesx2018The best most consoling book for children on the 
subject or bereavementx2026a joy to readx2019 The Independent on Sunday   
Product: Mog on Fox Night - Description: Praise for Mog the Forgetful 
Catx2018Grandparents are likely to get as much fun out of seeing it again as the 
new generation of fans just learning to readx2019 Choice Magazinex2018A lovely 
book for all Mogfanciersx2019 The ObserverPraise for Goodbye Mogx2018Kerrx2019s 
warmth humour and honesty make this an engaging introduction to a difficult 
topicx2019 Financial Timesx2018Believable amusing and movingx2019 Nursery 
Worldx2018A supremely sensitive storyx2019 The Timesx2018The best most consoling 
book for children on the subject of bereavementx2026a joy to readx2019 The 
Independent on Sunday  
  
### Response:  
  
Mog's Kittens - Description: Judith Kerr8217s best8211selling adventures of that 
endearing and exasperating cat Mog have entertained children for more than 30 
years Now even infants and toddlers  
```

## Compara√ß√£o das Respostas dos Modelos

| Produto                     | Modelo Original (Foundation Model)                                                                                       | Modelo com Fine-Tuning                                                                                           | Modelo com RAG                                                                                                   |
|-----------------------------|--------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| Girls Ballet Tutu Neon Blue | This is a beautiful tutu. It is made of high quality material and it is very comfortable. It is perfect for any occasion and it is very affordable. It is also very durable and it will last for a long time. It is also very easy to maintain and it is very easy to clean. It is also very easy to wear and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off. It is also very easy to put on and it is very easy to take off.                              | The perfect gift for the little ballerina in your life this tutu is made of soft  nylon tulle and is fully lined with a satin drawstring waistband                  | Dance tutu for girls ages 28 years. Perfect for dance practice, recitals and performances, costumes or just for fun. |
| Mog's Kittens               | The product is a pair of socks.                                                                                          | Praise for Mog the Forgetful Catx2018Grandparents are likely to get as much fun out of seeing it again as the new generation of fans just learning to readx2019  Choice Magazinex2018A lovely book for all Mogfanciersx2019 The  Observerx2018Kerrx2019s watercolours are full of humour and expressionx2019 Financial Timesx2018A lovely book for all  Mogfanciersx2019 The Observerx2018Kerrx2019s watercolours are full of humour and  expressionx2019 Financial Times                     | Judith Kerr8217s best8211selling adventures of that endearing and exasperating cat Mog have entertained children for more than 30 years Now even infants and toddlers    |
| The Prophet                 | The Prophet is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money. It is a product that helps you to find the best way to spend your money.                                      | The Prophet is a book of 26 prose poetry essays written in English by  LebaneseAmerican artist Kahlil Gibran 1883ndash1931 The Prophet has been  translated into over 20 languages and has sold more than 100 million copies  worldwide  | In a distant timeless place a mysterious prophet walks the sands At the moment of his departure he wishes to offer the people gifts but possesses nothing The people gather round each asks a question of the heart and the mans wisdom is his gift It is Gibrans gift to us as well for Gibrans prophet is rivaled in his wisdom only by the founders of the worlds great religions On the most basic topicsmarriage children friendship work pleasurehis words have a power and lucidity that in another era would surely have provoked the description divinely inspired Free of dogma free of power structures and metaphysics consider these poetic moving aphorisms a 20thcentury supplement to all sacred traditionsas millions of other readers already haveBrian BruyaThis text refers to theHardcoveredition|
| The Book of Revelation      | The Book of Revelation is a book of the New Testament of the Bible, and its title originated from the first word of the text in the Koine Greek: apokalypsis, meaning "unveiling" or "revelation". The author describes himself as "John" and does not identify himself as the son of Zebedee, the apostle John. The text is a letter to seven churches in the Roman province of Asia, and is a call to the churches to remain faithful to Jesus Christ, and individual letters to each church, with a promise of a swift punishment for Christian communities that are in a state of apostasy. The Book of Revelation is the final book of the New Testament and occupies a central place in Christian eschatology. By tradition, this prophecy was revealed by its author to the apostle John on the island of Patmos, and from its first readers, this prophecy has been accepted as of divine inspiration. The author of Revelation does not identify himself, but introduces his work as "the revelation of Jesus Christ", which he received "by an angel" from God.                                                     | The Book of Revelation is the last book of the New Testament and one of the most enigmatic and controversial works in Western literature It is a book of  apocalyptic prophecy that predicts the end of the world and the Last Judgment  The book is also known as the Apocalypse of John or simply the Apocalypse  | American Baptist pastor Bible teacher and writer Clarence Larkin was born October 28 1850 in Chester Delaware County Pennsylvania He was converted to Christ at the age of 19 and then felt called to the Gospel ministry but the doors of opportunity for study and ministry did not open immediately He then got a job in a bank When he was 21 years old he left the bank and went to college graduating as a mechanical engineer He continued as a professional draftsman for a while then he became a teacher of the blind Later failing health compelled him to give up his teaching career After a prolonged rest he became a manufacturer When he was converted he had become a member of the Episcopal Church but in 1882he became a Baptist and was ordained as a Baptist minister two years later He went directly from business into the ministry His first charge was at Kennett Square Pennsylvania his second pastorate was at Fox Chase Pennsylvania where he remained for 20 years He was not a premillennialist at the time of his ordination but his study of the Scriptures with the help of some books that fell into his hands led him to adopt the premillennialist position He began to make large wall charts which he titled Prophetic Truth for use in the pulpit These led to his being invited to teach in connection with his pastoral work in two Bible institutes During this time he published a number of prophetical charts which were widely circulated When World War I broke out in 1914 he was called on for addresses on The War and Prophecy Then God laid it on his heart to prepare a work on Dispensational Truth or Gods Plan and Purpose in the Ages containing a number of charts with descriptive matter He spent three years of his life designing and drawing the charts and preparing the text The favorable reception it has had since it was first published in 1918 seems to indicate that the world was waiting for such a book Because it had a large and wide circulation in this and other lands the first edition was soon exhausted It was followed by a second edition and then realizing that the book was of permanent value Larkin revised it and expanded it printing it in its present form He went to be with the Lord on January 24 1924This text refers to thePaperbackedition|

---
üìå **Conclus√£o:**

| Produto                   | Foundation Model | Fine-Tuned Model | RAG Model |
|---------------------------|-----------------|-----------------|-----------|
| **Girls Ballet Tutu** | Gen√©rica e irrelevante | Melhorada, mas imprecisa | Extra√≠da diretamente da base de dados |
| **Mog's Kittens** | Errado | Parcialmente correto | Descri√ß√£o original extra√≠da da base |


  - O modelo com fine-tuning apresenta respostas mais detalhadas e relevantes em 
    compara√ß√£o ao modelo original.
  - O modelo com RAG utiliza a base de dados indexada para fornecer respostas mais 
    precisas e contextualizadas, mas est√° limitado aos dados previamente indexados.
  - O modelo original (Foundation Model) apresenta respostas gen√©ricas e, em alguns 
    casos, completamente irrelevantes, como no exemplo de "Mog's Kittens", onde a 
  - resposta foi "The product is a pair of socks".
  - O modelo com fine-tuning demonstra uma melhoria significativa na qualidade das 
    respostas, mas ainda apresenta redund√¢ncias ou informa√ß√µes desnecess√°rias em 
    alguns casos.
  - O modelo com RAG √© altamente dependente da qualidade e abrang√™ncia dos dados 
    indexados. Ele √© ideal para cen√°rios onde as informa√ß√µes relevantes est√£o 
    contidas em uma base de dados espec√≠fica.